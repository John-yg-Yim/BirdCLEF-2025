{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f2406332",
   "metadata": {},
   "source": [
    "# üê¶ **Kaggle BirdCLEF+2025 Competition**\n",
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ **Final Results**\n",
    "| Stage                                 | Public Score | Private Score | Submitted | Final Leaderboard Rank |\n",
    "|--------------------------------------|:------------:|:-------------:|:---------:|:----------------------:|\n",
    "| 1. Applied initial 5-second segment  |     0.793     |     0.803     |     -     |           -            |\n",
    "| 2. Condition-based segment selection |     0.809     |     0.821     |     -     |           -            |\n",
    "| 3. Model change + hyperparameter tuning |  0.825    |     0.841     |     -     |           -            |\n",
    "| 4. Model architecture update         |     0.835     |   **0.847**   |    ‚úì      |           -            |\n",
    "| 5. Public model ensemble             |     0.878     |   **0.893**   |     -     |           -            |\n",
    "| 6. Post-processing adjustments       |     0.881     |   **0.891**   |    ‚úì      |   **507 / 2026**       |\n",
    "\n",
    "---\n",
    "\n",
    "### üìå **Competition Information**\n",
    "- **Title**: [BirdCLEF+2025](https://www.kaggle.com/competitions/birdclef-2025)\n",
    "- **Organizers**: Cornell Lab of Ornithology, LifeCLEF\n",
    "- **Period**: Mar 10, 2025 ‚Äì Jun 5, 2025 (UTC)\n",
    "- **Number of Teams**: 2,026\n",
    "- **Evaluation Metric**: Macro-average ROC-AUC across classes (excluding classes not present in the test set)\n",
    "- **Submission Format**:\n",
    "  - Predict presence probabilities for 206 species for every 5-second audio segment\n",
    "  - Maximum of 2 code submissions; final rank is based on the Private Score\n",
    "- **Project Objective**: Detect and classify 206 bird species from 60-second soundscape recordings in natural environments\n",
    "- **Execution Constraints**:\n",
    "  - Internet disabled\n",
    "  - Code notebook must run within 90 minutes (CPU) or 1 minute (GPU)\n",
    "  - Use of external public datasets and pretrained models is allowed\n",
    "  - Required output filename: `submission.csv`\n",
    "\n",
    "---\n",
    "\n",
    "### üë• **Team and My Role**\n",
    "- **Participation Period**: May 2, 2025 ‚Äì Jun 5, 2025 (UTC)\n",
    "- **Team Composition**: Participated as a team of 5 members\n",
    "- **My Role**: Team Leader (led the entire pipeline including preprocessing, model design, experimentation, and ensembling)\n",
    "  - Designed the preprocessing strategy and criteria for selecting segments\n",
    "  - Architected the model and ensemble framework\n",
    "  - Led post-processing and class weight adjustment strategies\n",
    "  - Directed experimentation and implemented the end-to-end pipeline\n",
    "\n",
    "---\n",
    "\n",
    "### üõ†Ô∏è **Technologies & Libraries Used**\n",
    "- **Language**: Python 3.11\n",
    "- **Deep Learning Framework**: PyTorch\n",
    "- **Model Architecture & Backbones**: `timm` (EfficientNet, NFNet, SE-ResNeXt)\n",
    "- **Audio Processing**: `librosa`, `torchaudio` (MelSpectrogram, AmplitudeToDB)\n",
    "- **Data Handling**: `numpy`, `pandas`, `scikit-learn`\n",
    "- **Visualization**: `matplotlib`, `seaborn`\n",
    "\n",
    "---\n",
    "\n",
    "### üíª **Development Environment**\n",
    "- **Operating System**: Ubuntu 24.04\n",
    "- **IDE / Editor**: VSCode\n",
    "- **Hardware**\n",
    "  - CPU: Ryzen 9 5900X\n",
    "  - RAM: 16GB * 2 = 32GB\n",
    "  - GPU: Nvidia 4070Ti (12GB VRAM)\n",
    "- **Additional Environment**: Kaggle Notebooks\n",
    "  - Multiple models trained and tested in parallel to maximize time efficiency\n",
    "\n",
    "---\n",
    "\n",
    "### üìÇ **Dataset Overview**\n",
    "- **`train_audio`**\n",
    "  - ~20,000 audio clips (5‚Äì60 seconds each)\n",
    "  - Contains sounds from birds (Aves), amphibians (Amphibia), mammals (Mammalia), and insects (Insecta)\n",
    "  - Sourced from Xeno-canto, iNaturalist, and CSA (32‚ÄØkHz, OGG format)\n",
    "  - Weakly labeled data\n",
    "\n",
    "- **`train_soundscapes`**\n",
    "  - ~10,000 unlabeled 60-second recordings\n",
    "  - Captured at locations similar to test environments\n",
    "\n",
    "- **`test_soundscapes`**\n",
    "  - ~700 one-minute recordings for evaluation\n",
    "  - Automatically placed in the test directory (32‚ÄØkHz, OGG format)\n",
    "\n",
    "- **`train.csv`**\n",
    "  - Metadata including `primary_label`, `secondary_labels`, `latitude`, `longitude`, `author`, `filename`, `rating`, and `collection`\n",
    "\n",
    "- **`sample_submission.csv`**\n",
    "  - `row_id` (soundscape_id_end_time)\n",
    "  - Expected probability predictions for 206 species\n",
    "\n",
    "- **`taxonomy.csv`**\n",
    "  - Species ID, scientific name, and class category (Aves / Amphibia / Mammalia / Insecta)\n",
    "\n",
    "- **`recording_location.txt`**\n",
    "  - Descriptions of recording locations in the El Silencio Nature Reserve, Colombia"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b828aae6",
   "metadata": {},
   "source": [
    "### üìñ **Baseline Code (Public Score: 0.761)**\n",
    "\n",
    "- **Preprocessing Baseline**: [Transforming audio to mel-spec (Kadir Candisolu)](https://www.kaggle.com/code/kadircandrisolu/transforming-audio-to-mel-spec-birdclef-25)\n",
    "    - **Config**\n",
    "        - Sampling rate: 32‚ÄØkHz\n",
    "        - Segment length: 5 seconds\n",
    "        - Target shape: 256 √ó 256\n",
    "        - Mel-spectrogram parameters: `n_fft=1024`, `hop_length`, `n_mels=128`, `fmin=50`, `fmax=14000`\n",
    "    - **Workflow**\n",
    "        1. Extract the center 5-second segment\n",
    "        2. If shorter than target length, repeat and pad the audio\n",
    "        3. Convert to mel-spectrogram using `librosa.feature.melspectrogram` ‚Üí dB scale ‚Üí min-max normalization [0‚Äì1]\n",
    "        4. Resize to (256, 256) and save as `.npy` file using `cv2.resize`\n",
    "\n",
    "- **Train Baseline**: [EfficientNet-B0 PyTorch Train BirdCLEF-25 (Kadir Candisolu)](https://www.kaggle.com/code/kadircandrisolu/efficientnet-b0-pytorch-train-birdclef-25)\n",
    "    - **Augmentation**\n",
    "        - SpecAugment (time & frequency masking)\n",
    "        - Random brightness and contrast\n",
    "        - Mixup regularization\n",
    "    - **Model**\n",
    "        - Backbone: `timm.create_model('efficientnet_b0', pretrained=True, in_chans=1)`\n",
    "        - Classifier: `AdaptiveAvgPool2d ‚Üí Dropout(0.2) ‚Üí Linear(num_classes)`\n",
    "    - **Training Settings**\n",
    "        - Loss: `BCEWithLogitsLoss`\n",
    "        - Optimizer: `AdamW`\n",
    "        - Scheduler: `CosineAnnealingLR`\n",
    "        - Cross-validation: `StratifiedKFold(n_splits=5)`\n",
    "\n",
    "- **Inference Baseline**: [EfficientNet-B0 PyTorch Inference BirdCLEF-25 (Kadir Candisolu)](https://www.kaggle.com/code/kadircandrisolu/efficientnet-b0-pytorch-inference-birdclef-25)\n",
    "    - **Config**: Same as training configuration\n",
    "    - **Inference Workflow**\n",
    "        1. Split 60-second audio into 5-second segments ‚Üí `total_segments = len / 5`\n",
    "        2. Convert to mel-spectrogram and resize to 256√ó256\n",
    "        3. Predict with each fold and apply `sigmoid(outputs)` ‚Üí **soft-voting**\n",
    "    - **Submission**\n",
    "        - Fill predictions into `sample_submission.csv` format and save as `submission.csv`\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f64fa79",
   "metadata": {},
   "source": [
    "### **1. Segment Timing Adjustment & Post-processing Added (PB 0.761 ‚Üí 0.793 | PV 0.803)**\n",
    "- **Segment Position Change**\n",
    "    - **Before**: Center 5-second segment (baseline)\n",
    "    - **After**: First 5 seconds of the audio file\n",
    "- **Post-processing (Temporal Smoothing)**\n",
    "    - **Edge segments**: Weighted average with previous/next segments (0.8 / 0.2)\n",
    "    - **Inner segments**: Weighted average of previous/current/next segments (0.2 / 0.6 / 0.2)\n",
    "\n",
    "---\n",
    "\n",
    "### **2. Condition-Based Segment Selection (PB 0.793 ‚Üí 0.809 | PV 0.803 ‚Üí 0.821)**\n",
    "1. **Handling Short Audio**\n",
    "    - If length < 5s: repeat and pad to create a valid 5-second segment\n",
    "2. **CSA Recording Exception** (contains human voice by default)\n",
    "    - Based on listening tests, files starting with `CSA` consistently include human speech\n",
    "    - For these files, a fixed 2‚Äì7 second segment (less human noise) is used\n",
    "3. **Valid Segment Detection**\n",
    "    - Criteria: Frequency ‚â• 2kHz, threshold ‚â• -27.5dB\n",
    "    - Apply 5-second sliding window at 1-second steps\n",
    "    - Choose the window with the most valid frames; if none found, default to 2‚Äì7s segment\n",
    "\n",
    "---\n",
    "\n",
    "### **3. Model & Hyperparameter Improvements (PB 0.809 ‚Üí 0.825 | PV 0.821 ‚Üí 0.841)**\n",
    "- **Backbone Model Update**\n",
    "    - **Before**: `efficientnet_b0`\n",
    "    - **After**: `tf_efficientnetv2_s.in21k_ft_in1k`\n",
    "- **Mel-Spectrogram Hyperparameters**\n",
    "\n",
    "| Parameter      | Before | After |\n",
    "|----------------|:------:|:-----:|\n",
    "| `n_fft`        | 1024   | 2048  |\n",
    "| `hop_length`   | 512    | 512   |\n",
    "| `n_mels`       | 128    | 512   |\n",
    "| `fmin`         | 50     | 20    |\n",
    "| `fmax`         | 14000  | 16000 |\n",
    "\n",
    "- **Loss Function**\n",
    "    - **Before**: `BCEWithLogitsLoss`\n",
    "    - **After**: `FocalLossBCE`\n",
    "- **Post-processing Weight Change**\n",
    "    - **Before**: Edge segment smoothing = 0.8 / 0.2\n",
    "    - **After**: Edge segment smoothing = 0.9 / 0.1\n",
    "\n",
    "---\n",
    "\n",
    "### **4. Model Architecture Modification (PB 0.825 ‚Üí 0.835 | PV 0.841 ‚Üí 0.847)**\n",
    "- **Decoupling Backbone and Head**\n",
    "    - **Before**: Used classifier layer embedded inside the backbone\n",
    "    - **After**: Set `features_only=True` to extract pure feature maps from the backbone\n",
    "- **Applied Custom Convolutional Head**\n",
    "    - Used `1√ó1 Conv` to enhance inter-channel interactions\n",
    "    - Classification head: `AdaptiveAvgPool ‚Üí Flatten ‚Üí Linear`\n",
    "- **Analysis**\n",
    "    - Fully utilizes pretrained features by cleanly separating backbone and head\n",
    "    - Conv head better captures spatial patterns for classification\n",
    "\n",
    "---\n",
    "\n",
    "### **[5. Public Model Ensemble (PB 0.835 ‚Üí 0.878 | PV 0.847 ‚Üí 0.893)](https://www.kaggle.com/code/johnyim1570/bird25-weightedblend-nfnet-seresnext-0-878)**\n",
    "- **Referenced Public Notebooks**\n",
    "    - üîó **Blending Logic**: [Bird25 | WeightedBlend | NFNet + ConvNeXtV2 | LB 0.860](https://www.kaggle.com/code/hideyukizushi/bird25-weightedblend-nfnet-convnextv2-lb-860) by [yukiZ](https://www.kaggle.com/hideyukizushi)\n",
    "    - üß™ **NFNet Model (PB 0.857)**: [Bird2025 | Single SED Model Inference [LB 0.857]](https://www.kaggle.com/code/i2nfinit3y/bird2025-single-sed-model-inference-lb-0-857) by [I2nfinit3y](https://www.kaggle.com/i2nfinit3y)\n",
    "    - üìò **SE-ResNeXt Model (PB 0.850)**: [Post-Processing with Power Adjustment for Low-Rank](https://www.kaggle.com/code/myso1987/post-processing-with-power-adjustment-for-low-rank) by [MYSO](https://www.kaggle.com/myso1987)\n",
    "\n",
    "- **Model Replacement**\n",
    "    - Replaced original models in the public WeightedBlend notebook with the two high-performing models above\n",
    "    - Applied inference-level ensemble\n",
    "\n",
    "- **Blending Weights**\n",
    "    - NFNet: 25%\n",
    "    - SE-ResNeXt: 75%\n",
    "\n",
    "---\n",
    "\n",
    "### **6. NFNet Post-processing Adjustment (PB 0.878 ‚Üí 0.881 | PV 0.893 ‚Üí 0.891)**\n",
    "- **Power Adjustment Hyperparameter Tuning**\n",
    "    - **Before**: Applied a fixed `exponent=2` to all tail columns ranked below `top_k=30`\n",
    "    - **After**: Split tail columns into 3 groups and applied different exponents:\n",
    "        - Rank 31‚Äì100: `exponent=2`\n",
    "        - Rank 101‚Äì150: `exponent=3`\n",
    "        - Rank >150: `exponent=4`\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8507edfd",
   "metadata": {},
   "source": [
    "## üß† **Conclusion & Reflections**\n",
    "\n",
    "After analyzing the BirdCLEF leaderboards from 2022 to 2024, a clear pattern emerged: the majority of participants experienced a **drop in Private Score compared to Public Score**, especially outside of the Silver medal range. Many discussions highlighted persistent concerns about **overfitting to the public test set**.\n",
    "\n",
    "The WeightedBlend ensemble (PB 0.878) was tuned by experimenting with different weight ratios (e.g., 0.5:0.5 ‚Üí 0.75:0.25), and further improved to PB 0.881 through post-processing adjustments. However, We suspected this performance gain was **likely overfitted to the public test set**.\n",
    "\n",
    "As a result, We adopted a two-fold final submission strategy:\n",
    "1. The best-performing model We trained independently (**PB 0.835**)\n",
    "2. The ensemble model with the highest public score (**PB 0.881**)\n",
    "\n",
    "However, once the private leaderboard was revealed, the outcome defied expectations. **All submissions scored higher on the Private Leaderboard**, and this trend was consistent across many teams. Surprisingly, the initial ensemble (PB 0.878) achieved **PV 0.893**, which aligned with **Bronze medal range** scores.\n",
    "\n",
    "### ‚ùó **Summary**\n",
    "- Assumptions based on historical leaderboard trends did **not apply** to this year's competition.  \n",
    "- A **conservative submission strategy**‚Äîrather than bold experimentation‚Äîled to a **drop in final ranking**.  \n",
    "- **Public Leaderboard Rank: 162 ‚Üí Private Leaderboard Rank: 507**\n",
    "\n",
    "---\n",
    "\n",
    "## üòì **Technical Limitations & Regrets**\n",
    "\n",
    "From my perspective, there were two core challenges that defined this competition:\n",
    "\n",
    "1. How to refine and effectively utilize the weakly labeled `train_audio` data.\n",
    "2. How to generate trustworthy pseudo-labels from `train_soundscapes` for semi-supervised learning.\n",
    "\n",
    "For challenge #1, We focused on the fact that **most bird sounds occur in high-frequency ranges**, and developed a heuristic that:\n",
    "> Filters 5-second audio segments where signals exceed -27.5‚ÄØdB above 2‚ÄØkHz.\n",
    "\n",
    "While this method contributed to some performance improvements, it soon plateaued. We concluded that it wasn‚Äôt reliable enough to generate pseudo-labels, and therefore **decided not to use `train_soundscapes` at all**.\n",
    "\n",
    "Midway through the competition, We discovered the concept of **MIL (Multiple Instance Learning)**:\n",
    "> This technique allows a model to learn from weak labels by focusing on the specific time segments where the positive class appears, which could solve both challenges above in a unified manner.\n",
    "\n",
    "Unfortunately, due to time constraints and submission errors in the final phase, We were **unable to finish and submit the MIL-based inference code**.\n",
    "\n",
    "---\n",
    "\n",
    "## üí° **Technical Growth & Takeaways**\n",
    "\n",
    "Despite the limitations, I gained several valuable experiences through this competition:\n",
    "\n",
    "- Designed and tested preprocessing strategies, model architectural changes, and post-processing techniques that directly led to performance improvements.\n",
    "- Developed an efficient workflow combining **Kaggle Notebooks and local GPU training**, which helped parallelize experiments and save time.\n",
    "- Learned about **knowledge distillation (Teacher‚ÄìStudent training)** for the first time while reviewing top solutions after the competition ended.\n",
    "- Gained practical insights into **data distribution analysis, overfitting detection, and submission strategy planning** for large-scale multi-label audio classification tasks."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
